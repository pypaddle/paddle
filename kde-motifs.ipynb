{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating KDEs of popular architectures for Motif usage / sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0_pca = PCA(n_components=15, whiten=False)\n",
    "feature0_data = feature0_pca.fit_transform(alexnet.features[0].weight.detach().numpy().reshape(-1, 11*11))\n",
    "feature0_kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(feature0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature3_pca = PCA(n_components=15, whiten=False)\n",
    "feature3_data = feature3_pca.fit_transform(alexnet.features[3].weight.detach().numpy().reshape(-1, 5*5))\n",
    "feature3_kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(feature3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Motif(object):\n",
    "    def __init__(self, data, downsample:int=15):\n",
    "        self._pca = PCA(n_components=downsample, whiten=False) if downsample else None\n",
    "        if self._pca is not None:\n",
    "            transformed_data = self._pca.fit_transform(data)\n",
    "            self._kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(transformed_data)\n",
    "        else:\n",
    "            self._kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(data)\n",
    "    \n",
    "    def sample(self):\n",
    "        kde_sample = self._kde.sample(1)\n",
    "        return kde_sample if self._pca is None else self._pca.inverse_transform(self._kde.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif0 = Motif(alexnet.features[0].weight.detach().numpy().reshape(-1, 11*11))\n",
    "motif3 = Motif(alexnet.features[3].weight.detach().numpy().reshape(-1, 5*5))\n",
    "motif6 = Motif(alexnet.features[6].weight.detach().numpy().reshape(-1, 3*3), downsample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vs random normal\t\t 1.0252208\n",
      "Sample vs random uniform\t 0.3491165\n",
      "Sample vs uniform(-0.8,0.8)\t 0.2294704\n",
      "Sample vs uniform(-0.4,0.4)\t 0.0689206\n",
      "Feature0 vs Feature0\t\t 0.031253207\n",
      "Feature0 vs Feature3in0\t\t 0.020539014\n",
      "Feature3 vs Feature0in3\t\t 0.100704215\n",
      "Feature6 vs Feature6\t\t 0.08165892\n",
      "Feature6 vs uniform(-0.4,0.4)\t 0.09314346\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "repetitions = 1000\n",
    "sample_feature0 = lambda: feature0_pca.inverse_transform(feature0_kde.sample(1))\n",
    "sample_feature3 = lambda: feature3_pca.inverse_transform(feature3_kde.sample(1))\n",
    "print('Sample vs random normal\\t\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(1, 121).normal_()) for _ in range(repetitions)]))\n",
    "print('Sample vs random uniform\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(1, 121).uniform_()) for _ in range(repetitions)]))\n",
    "print('Sample vs uniform(-0.8,0.8)\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(1, 121).uniform_(-0.8,0.8)) for _ in range(repetitions)]))\n",
    "print('Sample vs uniform(-0.4,0.4)\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(1, 121).uniform_(-0.4,0.4)) for _ in range(repetitions)]))\n",
    "print('Feature0 vs Feature0\\t\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(sample_feature0())) for _ in range(repetitions)]))\n",
    "\n",
    "sample_feature3in0 = lambda: feature0_pca.inverse_transform(feature3_kde.sample(1))\n",
    "sample_feature0in3 = lambda: feature3_pca.inverse_transform(feature0_kde.sample(1))\n",
    "print('Feature0 vs Feature3in0\\t\\t', np.mean([loss(torch.FloatTensor(sample_feature0()), torch.FloatTensor(sample_feature3in0())) for _ in range(repetitions)]))\n",
    "print('Feature3 vs Feature0in3\\t\\t', np.mean([loss(torch.FloatTensor(sample_feature3()), torch.FloatTensor(sample_feature0in3())) for _ in range(repetitions)]))\n",
    "\n",
    "print('Feature6 vs Feature6\\t\\t', np.mean([loss(torch.FloatTensor(motif6.sample()), torch.FloatTensor(motif6.sample())) for _ in range(repetitions)]))\n",
    "print('Feature6 vs uniform(-0.4,0.4)\\t', np.mean([loss(torch.FloatTensor(motif6.sample()), torch.FloatTensor(1, 9).uniform_(-0.4,0.4)) for _ in range(repetitions)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fba7e0d5190>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(sample_feature0().reshape(11, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_parameter_requires_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "disable_parameter_requires_grad(alexnet)\n",
    "num_classes = 10\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# download and transform train dataset\\nmnist_train_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('/media/data/set/mnist/',\\n    download=True,\\n    train=True,\\n    transform=train_transform),\\n    batch_size=batch_size,\\n    shuffle=True)\\n\\n# download and transform test dataset\\nmnist_test_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('/media/data/set/mnist/',\\n    download=True,\\n    train=False,\\n    transform=test_transform),\\n    batch_size=batch_size,\\n    shuffle=True)\\n\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 224\n",
    "batch_size = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  # first, convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize inputs for cifar\n",
    "    #transforms.Normalize((0.1307,), (0.3081,))  # normalize inputs for mnist\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),  # first, convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #transforms.Normalize((0.1307,), (0.3081,))  # normalize inputs for mnist\n",
    "])\n",
    "\n",
    "cifar10_dataset_root = '/media/data/set/cifar10'\n",
    "cifar10_train_set = datasets.CIFAR10(root=cifar10_dataset_root, train=True, download=True, transform=train_transform)\n",
    "cifar10_test_set = datasets.CIFAR10(root=cifar10_dataset_root, train=False, download=True, transform=test_transform)\n",
    "cifar10_train_loader = torch.utils.data.DataLoader(cifar10_train_set, batch_size=batch_size, shuffle=True)\n",
    "cifar10_test_loader = torch.utils.data.DataLoader(cifar10_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "# download and transform train dataset\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/media/data/set/mnist/',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=train_transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "# download and transform test dataset\n",
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/media/data/set/mnist/',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=test_transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 1.4601 Acc: 0.4736\n",
      "val Loss: 0.8935 Acc: 0.6844\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 1.3626 Acc: 0.5112\n",
      "val Loss: 0.8448 Acc: 0.6997\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 1.3380 Acc: 0.5237\n",
      "val Loss: 0.7918 Acc: 0.7240\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 1.3314 Acc: 0.5242\n",
      "val Loss: 0.8106 Acc: 0.7157\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 1.3227 Acc: 0.5257\n",
      "val Loss: 0.8124 Acc: 0.7131\n",
      "\n",
      "Training complete in 33m 34s\n",
      "Best val Acc: 0.724000\n"
     ]
    }
   ],
   "source": [
    "params_to_update = []\n",
    "for name, param in alexnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=0.01, momentum=0.5)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "alexnet.to(device)\n",
    "\n",
    "dataloaders_dict = {\n",
    "    'train': cifar10_train_loader,\n",
    "    'val': cifar10_test_loader\n",
    "}\n",
    "\n",
    "model_ft, hist = train_model(\n",
    "    alexnet,\n",
    "    dataloaders_dict,\n",
    "    loss_func,\n",
    "    optimizer,\n",
    "    num_epochs=5,\n",
    "    is_inception=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 64, 5, 5)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_alexnet_features3_weight = np.copy(model_ft.features[3].weight)\n",
    "trained_alexnet_features3_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif3.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 64, 5, 5)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_features3_weight = np.stack([np.stack([motif3.sample().reshape(5, 5) for _ in range(64)]) for _ in range(192)])\n",
    "sampled_features3_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign samples feature weight to model\n",
    "disable_parameter_requires_grad(model_ft)\n",
    "model_ft.features[3].weight = nn.Parameter(torch.FloatTensor(sampled_features3_weight), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the succeeding layer to be learnable\n",
    "model_ft.features[6].weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.6768 Acc: 0.0956\n"
     ]
    }
   ],
   "source": [
    "# Re-validate model\n",
    "model = model_ft\n",
    "optimizer = torch.optim.SGD(params_to_update, lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "model.eval()  # Set model to evaluating-stage\n",
    "\n",
    "dataloader = cifar10_test_loader\n",
    "\n",
    "# Iterate over data.\n",
    "for inputs, labels in dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(dataloader.dataset)\n",
    "epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format('Validation', epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to update:\n",
      "[torch.Size([384, 192, 3, 3]), torch.Size([10, 4096]), torch.Size([10])]\n",
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 2.3513 Acc: 0.0948\n",
      "val Loss: 2.3446 Acc: 0.0930\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.3516 Acc: 0.0942\n",
      "val Loss: 2.3446 Acc: 0.0930\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 2.3510 Acc: 0.0941\n",
      "val Loss: 2.3446 Acc: 0.0930\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 2.3511 Acc: 0.0941\n",
      "val Loss: 2.3446 Acc: 0.0930\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 2.3517 Acc: 0.0943\n",
      "val Loss: 2.3446 Acc: 0.0930\n",
      "\n",
      "Training complete in 48m 12s\n",
      "Best val Acc: 0.093000\n"
     ]
    }
   ],
   "source": [
    "model = model_ft\n",
    "\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "print('Parameters to update:')\n",
    "print([p.shape for p in params_to_update])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_ft2, hist2 = train_model(\n",
    "    model,\n",
    "    dataloaders_dict,\n",
    "    loss_func,\n",
    "    optimizer,\n",
    "    num_epochs=5,\n",
    "    is_inception=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda (paddle)",
   "language": "python",
   "name": "paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
